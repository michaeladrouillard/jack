---
title: "We Are All Trying to Describe Something And We Don't Have the Words"
subtitle: "The Jack Antonoffification of Pop Music?"
author: 
  - Michaela Drouillard
thanks: "Code and data are available at: LINK."
date: today
abstract: "In this essay, I will be exploring Jack Antonoff's production style using data from the Spotify API. Drawing from an interview with Caleb Gamman, a guy who went viral for guessing which song on Taylor Swift's Midnights album was produced by Antonoff within seconds, I will delve into the specific characteristics that define an Antonoff-y song, and assess whether or not the audio features defined by Spotify can capture this. I plan to lasso Glen McDonald,the Spotify engineer behind these features, into explaining them to me. There will be a Bayesian inference model. It will be a logistic regression. We do not know whether or not it will be good. Finally, I will armchair a little bit about the cultural significance of Antonoff's music and the reasons behind his ubiquity in the music industry."
format: pdf
number-sections: true
bibliography: references.bib
---

# Introduction

Sometime in early October of 2022 there was a chemical shift in my brain and I couldn't stop listening to Taylor Swift. I couldn't listen to anything other than Fearless or Red when deadlifting, walking to a friend's house, folding laundry, etc. I listened to so much Taylor in that month alone that she ended up as my #4 artist on Spotify Wrapped, which stops collecting its data for the year on October 31st. It didn't even capture what happened to me after Midnights dropped in November.

I was standing outside of the Garrison eating a slice of pizza with some friends, and one of them asked what music we had been listening to lately.

ME: "Midnights!"

ZACH: "Is it good?"

\[I glitched.\]

ME: "No!"

\["*No*?"\]

A STRANGER WALKING BY WHO HEARD EVERYTHING: "EXACTLY!!"

Midnights was produced by Swift and her longtime collaborator Jack Antonoff, who is the subject of this paper, and, I suspect, the reason behind both my blacking out, and the stranger's "Exactly".

Antonoff is prolific, to the extent that it is starting to bother people. He has produced and co-produced music for Lorde, Taylor Swift, Florence and the Machine, The Chicks, Clairo, The 1975, Grimes, Zayn, Pink, Lana Del Rey, Olivia Rodrigo, the Minions: The Rise of Gru Soundtrack, and more. All of this and his own bands too (Bleachers, and fun.). When I was getting brunch with a friend one Sunday and a Swift song came on, and she said "Jack Antonoff's gonna kill us all", we both laughed. What my friend was referring to, of course, is the fact that Jack Antonoff and his sound seems to be everywhere these days. As Andrew Marantz writes in the New Yorker: "When his band releases an album, the world responds politely. When he produces one by Lorde or Lana Del Rey or Taylor Swift, the world wobbles on its axis".

What I am trying to get to here is that I don't actually really mind Jack Antonoff's music, I kind of like it, but I still say his voice with the same tone that everyone else does. There's just something about him... this sense that he's everywhere... and that he's just such a nice guy... Why do music people seem to be a little annoyed with him? What is it about Antonoff music that is so Antonoff-y? Is the Jack Antonoff-fication of pop something we should be worried about?

To find an answer to this question, I turned to statistics, people who know more than me about describing music, and the overlord of engineered virality itself -- the Spotify API. There will be a Bayesian inference model. It will be a logistic regression. We do not know whether or not it will be good.

# An Interview with Caleb Gamman

Around the time that Midnights came out, I came across a video on Twitter of a guy, Caleb Gamman, guessing which songs on Midnights were produced by Antonoff within seconds. The tweet reads: "im able to instantly detect if jack antonoff worked on a song due to a visceral hatred of his production style". He nails it. Only one song on the album wasn't produced by Antonoff, and he guesses it.

I reached out to Gamman to ask him to explain Antonoff to me, only to find out, on the morning of the interview, that Gamman had released a 29 minute video essay on his Youtube Channel covering:

(you can skim the quote chunks)

1.  what makes an Antonoff song so Antonoff-y

    > I'm literally not an expert and of course I don't actually claim to always be able to 100 identify a Jack Antonoff song it's not like I've been practicing the way that I can tell if he's produced something is basically if it sounds to me like he's produced it I know that's annoying I will clarify first I think it's mostly in the vocal treatment and that's the thing that is the most off-putting to me but just every decision uh feels a little bit wrong to me I don't know of course if I wanted to be cringe in CinemaSins I could point out the Juno 6 Bass the perfectly stacked vocals the Juno 6 pad the wide and tight plate Reverb the Juno 6 sword the goo goo gaga the man hates treble unless it's just random crap swishing around everything gets low passed with thin 707 drum samples thin 808 drum samples then 909 drum samples thin Lin drum samples thin thin drum samples all the resonances the harmonics get removed everything is sort of squished down to the fundamental reduction in Timbre everything exists in a small frequency range everything becomes a tone it's an extremely consonant sound with no interplay between the different elements now that is a skill to do and it produces a cleaner sound and the only problem is I viscerally hate that sound

2.  his relationship to music growing up (some gems in here)

    > hing I got into drumming at the age oflike six so maybe as a drummer the drumsamples are of a fence I wasanachronistically raised on cassettes soyou know maybe I like the sound ofaudio and maybe that explains I tend tolike dissonance and in harmonic soundinterplay of elements Distortion noisethe Clashing of elements I imagine I'velistened to more SoundCloud in my lifethan radio so uh maybe I only want tohear amateur internet loser I'veliked over 10 000 songs on my Spotifyaccount uh and I can identify lot ofthem off the bat but I'm not that greatat identifying music generally so it'sjust uh compartmentalized mediaconsumption habit I first did musicproduction in primary school I mademashups and dubstep in it as a lateteenager I extensively used Juno 6emulators to make embarrassing synthwaveadmittedly that's probably not thesubconscious Association that he's goingfor lyrics aren't of much interest to meit's just a sound in the mix and somaybe I don't like the focus on vocalI've previously described it as a fightor flight sort of thing so maybe itliterally is tapping into some memory oftrauma I have an unusually small earcanal so maybe I'm just hearing the highend more acutely I was hired as ateenager to play drums on uh a lot ofJack Antonoff songs for a universityGlee Club performer so maybe I don'tlike it because of the embarrassment ofhaving ever been associated with a Gleecloth I liked the album 1989 but to makethe song out of the woods palette ofwool to me I had to mash it up with M83Midnight City and that got a dmcatakedown so maybe it's a personalVendetta and of course I have known anddo know people who like this music somaybe I secretly hate my friends andfamily but I don't know if any of thattells us anything uh I have a taste inmusic w

3.  An elegant and simple defense of taste

    > "i did use the phrase visceral hatred. but i did so only to describe my visceral hatred. i cant stand the sound of it. i hate it. and the way in which i hate it is viscerally. but i didnt go beyond that because it doesnt go beyond that. nothing against the guy, i like taylor swift's stuff. it's just subjective taste...

4.  His tongue in cheek response to some of Twitter's negative response to his video

    > ....Â \[it's just subjective taste\]. there's just nothing to engage with, so ive had to fabricate a reason for controversy. and what ive come up with is this. \*looks at phone\* the real reason iive been saying for years that i have a visceral reaction to jack antonoffs production style, the REAL reason is this \-- cause during the rollout cycle of the album midnights, i wanted to ascribe taylor swifts authorship to a man and diminish women's voices. so actually, i'm a sexist. so now thats out there, publishhed as facts, in the news. with all the other facts..."

5.  Basically a refutation to the entire premise of my interview

    > "what makes you so special and unique that you can tell the production of a song?" "nothing"

6.  Which was refuted even harder when I found a screenshot he tweeted of his tweet drafts in June 2021, one of them reading: "people often say to me:"caleb, what's wrong with jack antonoff?". don't ask questions"

![](images/Screen%20Shot%202023-04-03%20at%209.31.05%20PM.png)

It is a perfect video essay. I pestered him anyway.

> "In the treble mix of the song, like in the very upper end, like sort of above where the vocals are. He often really crushes that down. Like it's lower in volume than anything else, which is sort of a weird effect. Like, when you hear someone speaking, you hear a lot of that, like, noise in their voice. When you yourself are speaking, you hear less of it, right? You hear more of your own voice bouncing around in your head. And so it sort of creates this effect of -- **like he mixes his vocals the way it would sound to the person who's singing them. Which is sort of strange.** He often has no vocals on that upper end, which is very unusual, and then often there are random little bits of noise happening up there which is the sort of thing her likes to do."

Gamman is a fan of messier music.

"Jack Antonoff does a very good job of making stuff very clean. So you know, all of the notes that are happening simulaneously are perfectly stacked. They're like mathematically stacked .. Like, you have the harmonic spectrum where everything is like a multiple on top of each other. So that's like your notes on a keyboard. And if you played a keyboard note that was a little bit off where it's supposed to be, you're gonna create this interesting effect where these two notes sort of clash against each other and they wiggle around in terms of their tone because you've got these two harmonics happening that don't match. And Jack Antonoff avoids that quite strictly"

"His drums are low past, like the vocals, so you don't get like the tinny high end in the drums or whatever because that tinny high end in the drums is like a bunch of random harmonics all over the place. So if you cut that out, you just get the tone of the drum. And then he achieves those effects using the same sounds like literally the same synthesizer on the same drum machine and stuff."

"If you used a cscope spectrogram sort of thing to look at a Jack Antonoff song, you're gonna see a lot of big lines. And then if you're listening to a song by someone else, you're gonna see like a bunch of sort of like static wiggling. It's gonna be really jagged because you've got a bunch of harmonics that don't exactly mix with each other, so they're stacking in all sorts of weird different ways"

# Numbers

TLDR of the Gamman interview is: Taste is subjective. People who can play instruments tend to be better at identifying and describing sounds. Cultural criticism is a hall of smoke and mirrors. Antonoff is everywhere.

Can we translate any of this into something observable? Can we quantify the Jack Antonoff-ification of pop music?

# Data {#sec-data}

I pulled the data using the Spotify API, which I accessed using the `spotifyr` package in R. I used the `get_artist_audio_features` and `get_track_audio_analysis functions` function to acquire data on Lorde, Taylor Swift, St. Vincent, Lana Del Rey, The Chicks, Florence and the Machine, and Bleachers discographies.

### Artist Selection

I chose these artists because they've produced multiple albums, and at least one album with Antonoff, as opposed to just individual songs. I chose the Chicks because I figured that collaborating with Antonoff after an extended gap in a longer spanning career than the other artists might offer some interesting results. The data points at beginning of their career reflect music that existed in a different time from the other artists in this dataset.

### Data Cleaning

I joined these data sets using the `rbind` function in base R, and created an additional variable called "jack", which contains a 1 if Jack Antonoff produced or co-produced the song, and a 0 if he had nothing to do with it. I got the information on which songs Antonoff did or did not produce from the "Jack Antonoff Production Discography" Wikipedia page.

### Spotify's API

So what do any one these features mean?

The Spotify API returns JSON metadata directly from the Spotify Data Catalogue.

#### Audio Features Dataset

The `get_artist_audio_features` provides a general profile of each song, according to variables developped by spotify, which makes for simple comparisons across artists.

However, it's hard to gauge exactly what variables like "danceability" or "valence" mean, even using the documentation. For instance, "valence" is described as "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)". "Danceability" is described as "how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable". Neither of these definitions fully cover what metrics and thresholds go into something like "danceability". However, whatever these features capture is easier to compare across artists in visualizations than more granular details about bars, beats, duration, and tatums[^1].

[^1]: \-- a word I just learned, which refers to "the lowest regular pulse train that a listener intuitively infers from the timing of perceived musical events". The full documentation for each variable included in this study is available in the Appendix

This is pretty vague. Especially valence -- is the positivity related to lyrics, or something else? I compared the valence scores of classical music tracks to see if this score is based on lyrics, or the audio itself. Jupiter's valence score is 326.78% higher than Mars. This not only indicates the valence is based on audio features other than lyrical content, but that it is in fact capturing a difference between the two songs' profiles.

```{r}

#| echo: false
#| warning: false
library(dplyr)

sub_planets<- read.csv(here::here("inputs/data/subset_planets.csv"))
sub_planets |>
  select(artist_name, valence, danceability, track_name) |>
  head() |>
  knitr::kable(
    col.names = c("Artist Name", 
                  "Valence", 
                  "Danceability",
                  "Track Name")
 )


```

I looked at the valence measures on Flight of the Bumblebee, a song that's much faster and "upbeat" than Mars and Jupiter. What I found complicated my understanding of valence -- different performances of the same song had different valence scores. Some were arias, played by different instruments. These tend to have higher valence scores. The lowest recorded valence is 0.2410, and the highest is .9230. What gives?

```{r}
sub_bees<- read.csv(here::here("inputs/data/subset_bumblebee.csv"))
sub_bees |>
   select(artist_name, valence, danceability, album_release_year, track_name) |>
   head(17) |>
   knitr::kable(
     col.names = c("Artist Name", 
                   "Valence",
                   "Danceability",
                   "Album Release Year",
                   "Track Name")
  )



```

I asked Glen McDonald, a "data alchemist" (see: Principal Engineer) who worked at The Echo Nest, a machine listening start-up that was acquired by Spotify in \[YEAR\]. He still works at Spotify now.

Glen told me that Valence and Danceability we created by giving thousands of examples to college interns and asking them to tag whether a song was positive or gloomy, or danceable or un-danceable. Variables like energy or instrumentalness, on the other hand, were determined through machine listening techniques. The human subjectivity part only came in when it came to tweaking the training data. (My favourite example Glen gave was that the ML treated banjos like a human voice, giving bluegrass songs with banjos high Speechiness ratings. Because there were no banjos in the training data, they were registered as human).

Glen said that the way he's made use of these variables is by combing Energy and Valence to get a quadrant, which usually turns out "pretty good".

\[discuss the convo we had about how even two humans can have disagreement about a song's positivity, let alone human vs machine. i.e. an elliot smith song where the lyrics are depressing but the melody is happy. or a happy song but you know that the band died in a plane crash yesterday. etc.\]

The final audio features in this study data set contains the variables: "artist_name", "track_name", "energy", "danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "jack".

```{r}
#| echo: false
#| warning: false

#library(kableExtra)
#df |>
 # knitr::kable()

```

# "EDA"

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
```

```{r}
#| label: fig-energy
#| fig-cap: Energy Variable
#| echo: false
#| warning: false

library(lubridate)
library(tidyverse)
df <- read_csv(here::here("inputs/data/df.csv"))
df$jack <- factor(df$jack)
df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = energy, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap( ~ artist_name, ncol = 3)



```

```{r}
#| label: fig-danceability
#| fig-cap: Danceability Variable
#| echo: false
#| warning: false


df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = danceability, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080","#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap(~ artist_name, ncol = 3)




```

```{r}
#| label: fig-valence
#| fig-cap: Valence Variable
#| echo: false
#| warning: false


df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = valence, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080","#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap(~ artist_name, ncol = 3)





```

```{r}
#| label: fig-acousticness
#| fig-cap: Acousticness Variable
#| echo: false
#| warning: false


df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = acousticness, x = album_release_date, color = jack)) +
    geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080","#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap(~ artist_name, ncol = 3)





```

```{r}
#| label: fig-instrumentalness
#| fig-cap: Instrumentalness Variable
#| echo: false
#| warning: false


df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = instrumentalness, x = album_release_date, color = jack)) +
    geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080","#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap(~ artist_name, ncol = 3)





```

```{r}

#| label: fig-temp
#| fig-cap: Tempo Variable
#| echo: false
#| warning: false


df %>%
  filter(album_release_date_precision == "day") %>%
  mutate(album_release_date = ymd(album_release_date)) %>%
  ggplot(aes(y = tempo, x = album_release_date, color = jack)) +
    geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080","#FFB6C1")) +
  labs(color = "Has 'y' in jack?") +
  facet_wrap(~ artist_name, ncol = 3)



```

I'm gonna fix the axis labels later

```{r}

#| label: fig-quadvalence1
#| fig-cap: Valence and Energy Quadrant 
#| echo: false
#| warning: false



ggplot(data = df, aes(x = energy, y = valence)) + 
  geom_point(aes(color = as.factor(jack)), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$valence), color = "gray") +
  annotate("text", x = mean(df$energy), y = max(df$valence), 
           label = "High Energy, High Valence") +
  annotate("text", x = mean(df$energy), y = min(df$valence), 
           label = "High Energy, Low Valence") +
  annotate("text", x = min(df$energy), y = mean(df$valence), 
           label = "Low Energy, High Valence") +
  annotate("text", x = max(df$energy), y = mean(df$valence), 
           label = "Low Energy, Low Valence") 



```

```{r}

#| label: fig-quadvalence2
#| fig-cap: Valence and Energy Quadrant
#| echo: false
#| warning: false



ggplot(data = df, aes(x = energy, y = valence)) + 
  geom_point(aes(color = artist_name), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$valence), color = "gray") +
  annotate("text", x = mean(df$energy), y = max(df$valence), 
           label = "High Energy, High Valence") +
  annotate("text", x = mean(df$energy), y = min(df$valence), 
           label = "High Energy, Low Valence") +
  annotate("text", x = min(df$energy), y = mean(df$valence), 
           label = "Low Energy, High Valence") +
  annotate("text", x = max(df$energy), y = mean(df$valence), 
           label = "Low Energy, Low Valence") 
```

```{r}

#| label: fig-quaddanceability
#| fig-cap: Danceability and Energy Quadrant
#| echo: false
#| warning: false

ggplot(data = df, aes(x = energy, y = danceability)) + 
  geom_point(aes(color = artist_name), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$danceability), color = "gray") +
  annotate("text", x = mean(df$energy), y = max(df$danceability), 
           label = "High Energy, High Danceability") +
  annotate("text", x = mean(df$energy), y = min(df$danceability), 
           label = "High Energy, Low Danceability") +
  annotate("text", x = min(df$energy), y = mean(df$danceability), 
           label = "Low Energy, High Danceability") +
  annotate("text", x = max(df$energy), y = mean(df$danceability), 
           label = "Low Energy, Low Danceability") 

```

```{r}

#| label: fig-quaddanceability2
#| fig-cap: Danceability and Energy Quadrant
#| echo: false
#| warning: false


ggplot(data = df, aes(x = energy, y = danceability)) + 
  geom_point(aes(color = as.factor(jack)), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$danceability), color = "gray") +
  annotate("text", x = mean(df$energy), y = max(df$danceability), 
           label = "High Energy, High Danceability") +
  annotate("text", x = mean(df$energy), y = min(df$danceability), 
           label = "High Energy, Low Danceability") +
  annotate("text", x = min(df$energy), y = mean(df$danceability), 
           label = "Low Energy, High Danceability") +
  annotate("text", x = max(df$energy), y = mean(df$danceability), 
           label = "Low Energy, Low Danceability") 

```

Sure. These are fun quadrants.But it doesn't solve the Jack-iness underlying each of those points, which are scattered pretty broadly.

Let's throw a logistic regression at the problem.

# Model

Since we're trying to figure out whether these variables can tell us anything about what Jack Antonoff's sound is all about, a good way to figure that out is by seeing if any of these variables can help us predict what songs are produced by Jack. Logistic regressions like questions with binary outcomes.

This cannot be the right formula. $$
Pr(\theta | y) = \frac{Pr(y | \theta) Pr(\theta)}{Pr(y)} = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}}
$$ {#eq-bayes}

This model estimates the probability P of the outcome variable Y using predictor variables Xc. In other words, it estimates the probability that a song was produced by Jack Anotnoff using the Spotfiy variables. Which gives us information about the "research" "question": are these characteristics related to Jack's music?

The chart below gives us an idea of the mean value of each of these features for Jack and non-Jack song. A logistic regression will give us a better idea of how to weigh each of these features.

```{r}
#|: echo: false
#|: warning: false

data_reduced %>%
  group_by(jack) %>%
  summarise(across(-c(tempo, loudness), mean)) %>%
  pivot_longer(cols = -jack) %>%
  ggplot(aes(value, name, fill = factor(jack))) +
  geom_col(alpha = 0.8, position = "dodge") +
  labs(x = "Mean Value", y = "Feature")


```

One issue is that in our dataset, there are a lot more songs that aren't produced by Jack than songs that are:

```{r}
#|: echo: false
#|: warning: false

data_reduced %>%
  count(jack) %>%
  knitr::kable(col.names = c("Is it Jack?", "Number of Songs"))



```

So, we have to balance the classes.

Following Julia Silge's tutorial \[TK\], I used the tidymodels metapackage to split the training and testing data and make cross-validation samples. The `workflow_set()` function lets us hold different sets of feature engineering together and use tuning to evaluate them all at once. I need to evaluate two elements: a basic model, and a model that was downsampled using the `themis` package.

# Results

# Discussion

## Jack-iness and What Danceability Tells Us {#sec-first-point}

Using the Spotify Audio Features, you can't really gauge what Jack Antonoff's music sound like. You get a rough idea, but you don't get the same granularity that you would by simply DM'ing Caleb Gamman on Twitter and asking him to describe it you. \[ add gamman quote\]

According to this model, though, danceability is the most important feature in predicting whether or not a song was produced by Jack Antonoff. This is so hilarious. This was a feature that was invented in a lab 15 years ago. This number refers to the time a bunch of undergraduates were paid (hopefully?) to listen to a bunch of songs and tag whether they were danceable or not. It captures this je-ne-sais-quoi essence of music that connects with listeners on a visceral level. Even if it doesn't make YOUR body want to move, you could imagine it making SOMEBODY'S body want to move.

But does Jack Antonoff's music even make your body want to move? Let's put a pin in that.

Eliciting a visceral reaction doesn't cause career success -- factors in a career trajectory like network, connections, right time right place ism and other elements of biography all shape that sort of thing.

## Third discussion point

## Weaknesses and next steps

It only makes sense, of course, to hire a bunch of undergrads and get them to tell us whether or not a few thousand songs are danceable, and then build our own danceability metric, and see if we get the same results -- that danceability is the characteristic that helps us best understand what makes Jack, Jack.

A major weakness is cooked into the dataset. Since we're not in the studio, we can't know the degree of Jack-iness in each individual song. Especially when he takes on the role of a more backseat co-producer, as he did with Lana Del Rey's Did You Know There's a Tunnel Under Ocean Blvd? \[tk\].

\newpage

\appendix

# Appendix {.unnumbered}

### `get_artist_audio_features` documentation:

-   **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.

-   **danceability**: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.

-   **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.

-   **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.

-   **key**: The key the track is in. Integers map to pitches using standard [Pitch Class notation](https://en.wikipedia.org/wiki/Pitch_class). E.g. 0 = C, 1 = Câ¯/Dâ­, 2 = D, and so on. If no key was detected, the value is -1.

-   **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.

-   **loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.

-   **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.

-   **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

-   **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

# Additional details

\newpage

# References
